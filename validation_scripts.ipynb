{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from parameter_object import parameterClass\n",
    "%pylab inline\n",
    "#Imports .mdf files for providing ground-truth data.\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import numpy as np\n",
    "#sys.path.append('resources')\n",
    "#sys.path.append('final_software')\n",
    "import v2_functions as v2\n",
    "reload(v2)\n",
    "from scipy.ndimage import measurements, center_of_mass,filters\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import time\n",
    "import tifffile as tif_fn\n",
    "import csv\n",
    "import cPickle as pickle\n",
    "def read_ground_truth(file_name,time_pt):\n",
    "    f = open(str(file_name),'r')\n",
    "    txt = f.readlines()\n",
    "    track ={}\n",
    "    temp_x = []\n",
    "    temp_y = [];\n",
    "    temp_z = [];\n",
    "    name = []\n",
    "    #Populates new array if line is 'Track', and adds points if labelled 'Point'\n",
    "    for i in range(0,txt.__len__()):\n",
    "        line = txt[i].split(' ')\n",
    "        if line[0] == 'Track':\n",
    "            name.append(line[1])\n",
    "            if temp_x.__len__() >0:\n",
    "                track[int(name[-2])] = [temp_x, temp_y, temp_z]\n",
    "            temp_x = [];\n",
    "            temp_y = [];\n",
    "            temp_z = [];\n",
    "\n",
    "        elif line[0] == 'Point':        \n",
    "            temp_x.append(float(line[2]))\n",
    "            temp_y.append(float(line[3]))\n",
    "            temp_z.append(float(line[5]))\n",
    "    \n",
    "    \n",
    "    if temp_x.__len__() >0:\n",
    "        track[int(name[-1])] = [temp_x, temp_y, temp_z]\n",
    "    \n",
    "    #Empty array.\n",
    "    lengths = []\n",
    "    dots = []\n",
    "    for t in track:\n",
    "        lengths.append(track[t][0].__len__())\n",
    "    \n",
    "    #Finds the median point on the label.\n",
    "    average_height = np.median(np.array(lengths))\n",
    "    center_pt = int((average_height-1)/2)\n",
    "    \n",
    "    #Find the rectangles of interest.\n",
    "    rects = (time_pt, int(0), int(0), int(abs(par_obj.width)), int(abs(par_obj.height)))\n",
    "\n",
    "    #accepts and calculates track centers from tracks which are at least larger than half the median.\n",
    "    track_center = []\n",
    "    for t in track:\n",
    "        x = track[t][0]\n",
    "        y = track[t][1]\n",
    "        z = track[t][2]\n",
    "        if x.__len__() > center_pt:\n",
    "            #Always calculates the track center.\n",
    "            cent_x = x[int(round(x.__len__()/2,0))]\n",
    "            cent_y = y[int(round(x.__len__()/2,0))]\n",
    "            cent_z = z[int(round(x.__len__()/2,0))]\n",
    "        track_center.append([cent_y,cent_x,cent_z])\n",
    "\n",
    "    return track_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [0]\n",
      "tiff resolution not recognised\n",
      "Calculating features\n",
      "tiff resolution not recognised\n",
      "version1\n",
      "Calculating Features for Z:10 Timepoint: 1 File: 1\n",
      "tiff resolution not recognised\n",
      "version1\n",
      "Calculating Features for Z:1 Timepoint: 1 File: 1\n",
      "tiff resolution not recognised\n",
      "version1\n",
      "Calculating Features for Z:9 Timepoint: 1 File: 1\n",
      "tiff resolution not recognised\n",
      "version1\n",
      "Calculating Features for Z:20 Timepoint: 1 File: 1\n",
      "Time to train 0.548310995102\n",
      "Calculating features for evaluation\n",
      "tiff resolution not recognised\n",
      "version1\n",
      "Calculating Features for Timepoint: 1 All  Frames File: 1\n",
      "3.37812304497\n",
      "Calculating Features for Z: 2 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 3 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 4 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 5 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 6 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 7 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 8 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 11 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 12 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 13 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 14 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 15 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 16 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 17 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 18 Timepoint: 1 File: 1\n",
      "Calculating Features for Z: 19 Timepoint: 1 File: 1\n",
      "Calculating Features for Timepoint: 2 All  Frames File: 2\n",
      "8.12737798691\n",
      "Calculating Features for Z: 1 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 2 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 3 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 4 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 5 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 6 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 7 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 8 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 9 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 10 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 11 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 12 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 13 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 14 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 15 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 16 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 17 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 18 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 19 Timepoint: 2 File: 1\n",
      "Calculating Features for Z: 20 Timepoint: 2 File: 1\n",
      "Calculating Features for Timepoint: 3 All  Frames File: 3\n",
      "13.0353269577\n",
      "Calculating Features for Z: 1 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 2 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 3 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 4 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 5 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 6 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 7 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 8 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 9 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 10 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 11 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 12 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 13 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 14 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 15 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 16 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 17 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 18 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 19 Timepoint: 3 File: 1\n",
      "Calculating Features for Z: 20 Timepoint: 3 File: 1\n",
      "Calculating Features for Timepoint: 4 All  Frames File: 4\n",
      "22.6398911476\n",
      "Calculating Features for Z: 1 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 2 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 3 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 4 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 5 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 6 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 7 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 8 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 9 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 10 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 11 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 12 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 13 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 14 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 15 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 16 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 17 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 18 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 19 Timepoint: 4 File: 1\n",
      "Calculating Features for Z: 20 Timepoint: 4 File: 1\n",
      "Calculating Features for Timepoint: 5 All  Frames File: 5"
     ]
    }
   ],
   "source": [
    "\n",
    "#This is a class and function which takes over the print output in the absence of the gui.\n",
    "class intObject():\n",
    "    def __init__(self):\n",
    "        self.pa = None\n",
    "    def report_progress(self,string):\n",
    "        d=0\n",
    "        print(string)\n",
    "        \n",
    "    \n",
    "#Here we choose which of our experiments to run.\n",
    "for exp in [1]:\n",
    "    \n",
    "\n",
    "    \n",
    "    if exp == 1:\n",
    "        path = '/Users/dwaithe/Documents/collaborators/hailstoneM/4d_examples/Live_mdf/'\n",
    "        tif_out_path = '/Users/dwaithe/Documents/collaborators/hailstonem/out'\n",
    "    \n",
    "    #This is where we have the parameter sets.\n",
    "    for num_of_train in [1]:\n",
    "        #Number of times to repeat.\n",
    "        for bc in range(0,1):\n",
    "            int_obj = intObject()\n",
    "            \n",
    "            \n",
    "            ####If we have cross-validation.\n",
    "            #if exp ==1:\n",
    "            #    ss = ShuffleSplit(30, n_iter=1, test_size=15, train_size=num_of_train)\n",
    "            #for train_index, test_index in ss:\n",
    "                 #print(\"%s %s\" % (train_index, test_index))\n",
    "            ####\n",
    "            ###Remove the following in favour of the above cross-validation\n",
    "            train_index = [0]\n",
    "            test_index = [0]\n",
    "            ###\n",
    "    \n",
    "            temp_index = list(train_index)\n",
    "            #temp_index.extend(list(test_index))\n",
    "            #test_index = temp_index\n",
    "            print train_index, test_index\n",
    "            par_obj = parameterClass()\n",
    "            int_obj = intObject()\n",
    "            par_obj.gt_sum = {}\n",
    "            par_obj.gt_dense ={}\n",
    "            par_obj.f_matrix =[]\n",
    "            par_obj.o_patches=[]\n",
    "            par_obj.gt_sum = {}\n",
    "            par_obj.gt_dense ={}\n",
    "            par_obj.gt_file_array = []\n",
    "            par_obj.frames_2_load = {}\n",
    "            par_obj.gt_array =[]\n",
    "            \n",
    "            \n",
    "            ####May want to change these of course for the hessian and maxima finding.\n",
    "            #par_obj.min_distance\n",
    "            #par_obj.abs_thr\n",
    "            #par_obj.rel_thr\n",
    "            ####\n",
    "            \n",
    "            par_obj.feature_type = 'basic'\n",
    "\n",
    "            if exp == 1:\n",
    "                \n",
    "                nstr = [0]*1#Number of files to upload\n",
    "            \n",
    "            \n",
    "            datasets=['dense_arr','feat_arr','double_feat_arr','pred_arr','sum_pred','maxi_arr','pts','roi_stk_x','roi_stk_y','roi_stkint_x','roi_stkint_y']\n",
    "            par_obj.data_store={}\n",
    "            \n",
    "            for dataname in datasets:\n",
    "                par_obj.data_store[dataname]={}\n",
    "            \n",
    "            \n",
    "            #We initalize each of our images in our experiments.\n",
    "            for i in range(0,nstr.__len__()):\n",
    "                n = str(i).zfill(3)\n",
    "                if exp == 1:\n",
    "                    par_obj.ch_active =[0,1]\n",
    "                    par_obj.sigma =  float(4);\n",
    "                    pixel_cut_off_3D = 10.0\n",
    "                    par_obj.limit_time_pt = 5 #########HERE IS TO BE DELETED\n",
    "                    \n",
    "                    #i =0 is the first datafile. To add more do i==1 etc.\n",
    "                    if i == 0:\n",
    "                        ##############\n",
    "                        ############## These are the parameters of our first datafile.\n",
    "                        ##############\n",
    "                        \n",
    "                        #Input image.\n",
    "                        file_str = path+'/Concatenated Stacks_resized-1_every10.tif'\n",
    "                        #User training.\n",
    "                        ROI_str = path+'/Concatenated Stacks_resized-1_every10.quantiROI'\n",
    "                        \n",
    "                        #The ground-truth. Indexed by time-point.\n",
    "                        gt_time_pt = {}\n",
    "                        gt_time_pt[0]  = path+'Concatenated Stacks_resized-1_every10.tif.mdf'\n",
    "                        gt_time_pt[1]  = path+'Concatenated Stacks_resized-1_every10.tif2.mdf'\n",
    "                        gt_time_pt[2]  = path+'Concatenated Stacks_resized-1_every10.tif3.mdf'\n",
    "                        gt_time_pt[3]  = path+'Concatenated Stacks_resized-1_every10.tif4.mdf'\n",
    "                        gt_time_pt[4]  = path+'Concatenated Stacks_resized-1_every10.tif5.mdf'\n",
    "                        gt_time_pt[5]  = path+'Concatenated Stacks_resized-1_every10.tif6.mdf'\n",
    "                        gt_time_pt[6]  = path+'Concatenated Stacks_resized-1_every10.tif7.mdf'\n",
    "                        gt_time_pt[7]  = path+'Concatenated Stacks_resized-1_every10.tif8.mdf'\n",
    "                        gt_time_pt[8]  = path+'Concatenated Stacks_resized-1_every10.tif9.mdf'\n",
    "                        gt_time_pt[9]  = path+'Concatenated Stacks_resized-1_every10.tif10.mdf'\n",
    "                        gt_time_pt[10] = path+'Concatenated Stacks_resized-1_every10.tif11.mdf'\n",
    "                        gt_time_pt[11] = path+'Concatenated Stacks_resized-1_every10.tif12.mdf'\n",
    "                        gt_time_pt[12] = path+'Concatenated Stacks_resized-1_every10.tif13.mdf'\n",
    "                        gt_time_pt[13] = path+'Concatenated Stacks_resized-1_every10.tif14.mdf'\n",
    "                        gt_time_pt[14] = path+'Concatenated Stacks_resized-1_every10.tif15.mdf'\n",
    "                        \n",
    "                        par_obj.file_array.append(file_str)\n",
    "                        par_obj.gt_array.append(gt_time_pt)\n",
    "                \n",
    "\n",
    "            #Gets the dimensional information from the first file.\n",
    "            #Assumes they are all the same. May not be true, but Martin you changed the data_structure :-).\n",
    "            v2.import_data_fn(par_obj,[par_obj.file_array[0]])\n",
    "            try:\n",
    "                time_list = np.arange(0,par_obj.limit_time_pt)\n",
    "                par_obj.total_time_pt = par_obj.limit_time_pt\n",
    "            except:\n",
    "                time_list = np.arange(0,par_obj.total_time_pt)\n",
    "            \n",
    "            par_obj.time_pt_list = np.arange(0,par_obj.total_time_pt)\n",
    "            par_obj.max_file = nstr.__len__()\n",
    "            \n",
    "            \n",
    "            \n",
    "            par_obj.initiate_data_store()\n",
    "            \n",
    "            ########################\n",
    "            ########################\n",
    "            ########################\n",
    "           \n",
    "            \n",
    "            \n",
    "            print('Calculating features')\n",
    "            \n",
    "            #Import ROI for file.\n",
    "            ROI_file = pickle.load( open(ROI_str,\"rb\"))\n",
    "            par_obj.saved_dots = ROI_file['dots']\n",
    "            par_obj.saved_ROI = ROI_file['rect']\n",
    "            \n",
    "\n",
    "            #Need to update this with the imported saved points.\n",
    "            for b in train_index:\n",
    "\n",
    "    \n",
    "                #print \"Image_id\\t\",b, 'time taken: ',t2-t1\n",
    "                #Need to initialise are region for each image, which encompasses the whole plane.\n",
    "                for i in range(0,par_obj.saved_ROI.__len__()):\n",
    "                    rects = par_obj.saved_ROI[i]\n",
    "                    zslice = par_obj.saved_ROI[i][0]\n",
    "                    tpt = par_obj.saved_ROI[i][5]\n",
    "                    imno = par_obj.saved_ROI[i][6]\n",
    "                    v2.import_data_fn(par_obj,[par_obj.file_array[b]])\n",
    "                    par_obj.height = par_obj.ori_height\n",
    "                    par_obj.width = par_obj.ori_width\n",
    "                    \n",
    "                    \n",
    "                    #Load in image data\n",
    "                    v2.im_pred_inline_fn_new(par_obj, int_obj,[zslice],[tpt],[imno],threaded=False)\n",
    "                    #Create the input kernel 3-D map.\n",
    "                    v2.update_com_fn(par_obj,tpt,zslice,imno)\n",
    "                    #Take the annotations and the features.\n",
    "                    v2.update_training_samples_fn_new_only(par_obj,int_obj,rects)\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "            ##### Trains the forest. #####\n",
    "\n",
    "            par_obj.RF ={}\n",
    "            par_obj.RF[0] = ExtraTreesRegressor(par_obj.num_of_tree, max_depth=par_obj.max_depth, min_samples_split=par_obj.min_samples_split, min_samples_leaf=par_obj.min_samples_leaf, max_features=par_obj.max_features, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "\n",
    "            par_obj.pred_arr = {}\n",
    "            par_obj.sum_pred = {}\n",
    "\n",
    "\n",
    "            #Fits the data.\n",
    "            t3 = time.time()\n",
    "            par_obj.RF[0].fit(par_obj.f_matrix, par_obj.o_patches)\n",
    "            rf_score = par_obj.RF[0].score(par_obj.f_matrix, par_obj.o_patches)\n",
    "            t4 = time.time()\n",
    "            time_to_train = t4-t3\n",
    "            print 'Time to train',time_to_train\n",
    "            \n",
    "\n",
    "            ##### Evaluation of images. ######\n",
    "            \n",
    "            par_obj.final_prediction = {}\n",
    "            time_taken_to_calc_feat = []\n",
    "            time_taken_to_eval_trees = []\n",
    "            print 'Calculating features for evaluation'\n",
    "            par_obj.the_score = {}\n",
    "            par_obj.data_store['gt_pts'] ={}\n",
    "            for fileno in test_index:\n",
    "                par_obj.sum_pred ={}\n",
    "\n",
    "                t1 = time.time()\n",
    "                file_str = par_obj.file_array[fileno]\n",
    "                #We import the image.\n",
    "                v2.import_data_fn(par_obj,[file_str])\n",
    "                #Incase they are different.\n",
    "                par_obj.height = par_obj.ori_height\n",
    "                par_obj.width = par_obj.ori_width\n",
    "                zslice_list = np.arange(0,par_obj.max_zslices)\n",
    "                try:\n",
    "                    time_list = np.arange(0,par_obj.limit_time_pt)\n",
    "                    par_obj.total_time_pt = par_obj.limit_time_pt\n",
    "                except:\n",
    "                    time_list = np.arange(0,par_obj.total_time_pt)\n",
    "                \n",
    "                #Read in the frames.\n",
    "                v2.im_pred_inline_fn_new(par_obj, int_obj,zslice_list,time_list,[fileno],threaded=True)\n",
    "                #Evalute the forest for each slice.\n",
    "                v2.evaluate_forest_new(par_obj,int_obj,False,0,zslice_list,time_list,[fileno],threaded=False,b=b,arr='feat_arr')\n",
    "                t2 = time.time()\n",
    "                time_taken_to_calc_feat.append(t2-t1)\n",
    "                print 'Image_id: ',fileno,' time taken to calc features: ',time_taken_to_calc_feat[-1]\n",
    "                par_obj.frames_2_load = zslice_list\n",
    "                \n",
    "                \n",
    "                ###########\n",
    "                ########### Will export a particular timepoint to visualise.\n",
    "                ###########\n",
    "                #with tif_fn.TiffWriter(tif_out_path+'/exp_'+str(exp)+'_image-dense_0'+str(b)+'.tif', bigtiff=True) as tif:\n",
    "                #    for zslice in zslice_list:\n",
    "                #        tif.save(np.array(par_obj.data_store['pred_arr'][b][0][zslice]), compress=0)\n",
    "                #\n",
    "                ###########\n",
    "                ###########\n",
    "                ###########\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ########### \n",
    "                ###########This is where we start our peformance calculation.\n",
    "                ###########\n",
    "                \n",
    "                #This is the container for the ground-truths points that we load in.\n",
    "                par_obj.data_store['gt_pts'][fileno] = {}\n",
    "                for time_pt in par_obj.time_pt_list:\n",
    "                    v2.count_maxima(par_obj,time_pt,fileno)\n",
    "                    \n",
    "                    gt_pts = read_ground_truth(par_obj.gt_array[fileno][time_pt],time_pt)\n",
    "                    par_obj.data_store['gt_pts'][fileno][time_pt] = gt_pts\n",
    "\n",
    "                    ###########\n",
    "                    ########### Plot all the points.\n",
    "                    ###########\n",
    "                    #for y0,x0,z0 in par_obj.data_store['pts'][fileno][time_pt]:\n",
    "                    #    plot(x0,y0,'ro')\n",
    "                    #for y1,x1,z1 in par_obj.data_store['gt_pts'][fileno][time_pt]:\n",
    "                    #    plot(x1,y1,'bo')\n",
    "                    ###########\n",
    "                    ###########\n",
    "                    ###########\n",
    "\n",
    "                    rel = []\n",
    "                    gbl_min = []\n",
    "\n",
    "                    #Next we want to for every predicted point calculate the distance to the nearest ground-truth point.\n",
    "                    for y0,x0,z0 in par_obj.data_store['pts'][fileno][time_pt]:\n",
    "                        dist_arr = []\n",
    "                        for y1,x1,z1 in par_obj.data_store['gt_pts'][fileno][time_pt]:\n",
    "                            #Euclidean distance.\n",
    "                            dis = np.sqrt((y1-y0)**2 + (x1-x0)**2  +(z1-z0)**2)\n",
    "                            dist_arr.append(dis)\n",
    "                        #Nearest distance indice \n",
    "                        rel.append(np.argmin(dist_arr))\n",
    "                        #Nearest distance distance.\n",
    "                        gbl_min.append(np.min(dist_arr))\n",
    "\n",
    "                    pred_to_keep = []\n",
    "                    gt_to_keep = []\n",
    "                    dist_in_3D_pixels = []\n",
    "\n",
    "\n",
    "                    #Go through all of the relatives.\n",
    "                    for i in set(rel):\n",
    "                        #Find the indices where there a multiple nearest matches.\n",
    "                        ind = np.where(np.array(rel) == i)[0]\n",
    "                        #Finds the minimum index of the valid distances.\n",
    "                        dist_ind = ind[np.argmin(np.array(gbl_min)[ind])]\n",
    "                        min_dist = np.min(np.array(gbl_min)[ind])\n",
    "                        #pts which are nearest.\n",
    "                        pred_to_keep.append(dist_ind)\n",
    "                        gt_to_keep.append(rel[dist_ind])\n",
    "                        dist_in_3D_pixels.append(min_dist)\n",
    "\n",
    "\n",
    "                    ########### This is where we compare the two lists.\n",
    "                    ########### We can extract the x,y,z coordinates if we want more elaborate comparison than 3D Distance\n",
    "                    ########### \n",
    "                    figure()\n",
    "                    for y0,x0,z0 in par_obj.data_store['pts'][fileno][time_pt][pred_to_keep]:\n",
    "                        subplot(1,4,1)\n",
    "                        plot(x0,y0,'wo')\n",
    "                        subplot(1,4,2)\n",
    "                        plot(x0,z0,'wo')\n",
    "                    \n",
    "                    for y1,x1,z1 in np.array(par_obj.data_store['gt_pts'][fileno][time_pt])[gt_to_keep]:\n",
    "                        subplot(1,4,1)\n",
    "                        plot(x1,y1,'ko')\n",
    "                        subplot(1,4,2)\n",
    "                        plot(x1,z1,'ko')\n",
    "                    ###########\n",
    "                    ###########\n",
    "                    ###########\n",
    "\n",
    "                    #Trim the 3-D distances down to the ones within a distances. Default: pixel_cut_off_3D\n",
    "                    index_valid =  np.array(dist_in_3D_pixels) < pixel_cut_off_3D\n",
    "                    #Take those indices which match this from the nearest neighbour short-list.\n",
    "                    pred_to_keep_within_dist = np.array(pred_to_keep)[index_valid]\n",
    "                    gt_to_keep_within_dist = np.array(gt_to_keep)[index_valid]\n",
    "\n",
    "                    ########### This is for plotting the distances\n",
    "                    ###########\n",
    "                    ###########\n",
    "                    #Plot those remaining. Can also\n",
    "                    for y0,x0,z0 in par_obj.data_store['pts'][fileno][time_pt][pred_to_keep_within_dist]:\n",
    "                        subplot(1,4,3)\n",
    "                        plot(x0,y0,'ro')\n",
    "                        subplot(1,4,4)\n",
    "                        plot(x0,z0,'ro')\n",
    "                    for y1,x1,z1 in np.array(par_obj.data_store['gt_pts'][fileno][time_pt])[gt_to_keep_within_dist]:\n",
    "                        subplot(1,4,3)\n",
    "                        plot(x1,y1,'go')\n",
    "                        subplot(1,4,4)\n",
    "                        plot(x1,z1,'go')\n",
    "                    ###########\n",
    "                    ###########\n",
    "                    ###########\n",
    "\n",
    "                    num_gt = float(par_obj.data_store['gt_pts'][fileno][time_pt].__len__())\n",
    "                    num_hits = float(gt_to_keep_within_dist.__len__())\n",
    "                    c = 0\n",
    "                    for dots in par_obj.saved_dots:\n",
    "                        for dot in dots:\n",
    "                            c += 1\n",
    "\n",
    "                    ###########\n",
    "                    ########### These are the details we are interested in.\n",
    "                    ###########\n",
    "                    \n",
    "                    print '--------------------------------------------------------------------------------'\n",
    "                    print 'filename',par_obj.file_array[fileno]\n",
    "                    print 'time point',time_pt\n",
    "                    print 'number of training dots', c\n",
    "                    print 'number of predicted centres', par_obj.data_store['pts'][fileno][time_pt].shape[0]\n",
    "                    print 'ground_truths total', num_gt\n",
    "                    print 'predicted points total', float(par_obj.data_store['pts'][fileno][time_pt].shape[0])\n",
    "                    true_positives = num_hits\n",
    "                    print 'Valid centres (True positives)', true_positives\n",
    "                    false_positives = float( par_obj.data_store['pts'][fileno][time_pt].shape[0])-float(pred_to_keep_within_dist.shape[0])\n",
    "                    print 'Invalid centres (False positives)',false_positives \n",
    "                    false_negatives = float(num_gt)-float(pred_to_keep_within_dist.shape[0])\n",
    "                    print 'Non-matched ground-truth points (False negatives)', false_negatives\n",
    "                    ###########\n",
    "                    ###########\n",
    "                    ###########    \n",
    "            \n",
    "            \"\"\"\n",
    "            csvPath = \"/Users/dwaithe/Documents/collaborators/WaitheD/simulated Data/drosophilaBrain/large_dots/\";\n",
    "            output = '\\tPath of data:\\t',path,'\\time to train\\t',time_to_train,'\\tnumber of training images.\\t',num_of_train,'\\tthe score\\t',rf_score\n",
    "            with open(csvPath+'outputDataLastTrainFIN.csv', 'a') as csvfile:\n",
    "                spamwriter = csv.writer(csvfile,  dialect='excel')\n",
    "                spamwriter.writerow(output)\n",
    "            per_err =[]\n",
    "            for i,item in enumerate(test_index):\n",
    "                test= False\n",
    "                for c in train_index:\n",
    "                    if item  == c:\n",
    "                        test = True;\n",
    "                per_err.append((1-(abs(par_obj.gt_dense[item] - par_obj.final_prediction[item])/par_obj.gt_dense[item]))*100)\n",
    "                #output =  \"\\timage_id\\t\",item,\"\\ttime to process\\t\",time_taken_to_calc_feat[i],\"\\ttime to evaluate\\t\",time_taken_to_eval_trees[i],\"\\ttest\\t\",test,\"\\tgt_dense\\t\",par_obj.gt_dense[item],\"\\tfinal_predicion\\t\",par_obj.final_prediction[item],\"\\tper_err\\t\",per_err,\"\\tthe_score\\t\",par_obj.the_score[item]\n",
    "            output = \"\\toutput\\t\",np.average(per_err)\n",
    "            print output\n",
    "            with open(csvPath+'outputDataLastTrainFIN.csv', 'a') as csvfile:\n",
    "                    spamwriter = csv.writer(csvfile,  dialect='excel')\n",
    "                    spamwriter.writerow(output)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
